{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67697cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gnews import GNews\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from newspaper import Article\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a861f88",
   "metadata": {},
   "source": [
    "## Using GNews to collect news article headlines\n",
    "\n",
    "https://github.com/ranahaani/GNews   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfc9144",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# generate dates in 1 week intervals from March 01, 2024 to May 23rd, 2024\n",
    "dates = pd.date_range(start='3/1/2024', end='5/23/2024', freq='1W')\n",
    "# turn into tuples\n",
    "dates = [(date.year, date.month, date.day) for date in dates]\n",
    "\n",
    "\n",
    "# for each date interval, search and collect data\n",
    "for i in range(len(dates) - 1): # stop before the last date\n",
    "    \n",
    "    # create gnews object \n",
    "    news = GNews(language='en', \n",
    "                    country='US', \n",
    "                    start_date= dates[i], \n",
    "                    end_date=dates[i + 1], \n",
    "                    max_results=100, \n",
    "                   )\n",
    "    # get news \n",
    "    results = news.get_news('student protest')\n",
    "    \n",
    "    # if this is the first iteration\n",
    "    if i == 0: \n",
    "        df = pd.DataFrame(results)\n",
    "    \n",
    "    \n",
    "    # else add to existing df \n",
    "    else: \n",
    "        new_df = pd.DataFrame(results)\n",
    "        df = pd.concat([df, new_df])\n",
    "    \n",
    "    # increment i \n",
    "    i += 1\n",
    "\n",
    "end = time.time()\n",
    "print('time elapsed: ', end - start, ' seconds')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a47e7",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Our Approach: how do we determine if an article is about student protest?\n",
    "\n",
    "#### Part 1: \n",
    "\n",
    "If it contains three components: \n",
    "\n",
    "1.) has student related keywords\n",
    "\n",
    "2.) has protest related keywords\n",
    "\n",
    "3.) has context related keyword\n",
    "\n",
    "#### Part 2: \n",
    "\n",
    "For headlines with low score, get the first 250 characters of the article using the newspaper package to verify its topic. If the score is still low, we drop it. \n",
    "\n",
    "`student-related words:` \n",
    "- student*\n",
    "- college*\n",
    "- universit*\n",
    "- school*\n",
    "- campus*\n",
    "- faculty\n",
    "\n",
    "`protest-related words:`\n",
    "- activis*\n",
    "- protest*\n",
    "- encampment\n",
    "- demonstrat*\n",
    "- clash\n",
    "- divest*\n",
    "- \\*war\\*\n",
    "\n",
    "`context-related words:` \n",
    "- pro-palestin*\n",
    "- Israel-Hamas\n",
    "- Gaza\n",
    "- pro-israel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b1f65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywords = pd.read_excel('news and keywords.xlsx')\n",
    "keywords = keywords.loc[:,['Keywords (students)','Keywords (protest)', 'Keywords (context)']]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dataset \n",
    "df = pd.read_csv('larger_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0ce9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "keywords = pd.read_excel('news and keywords.xlsx')#['Keywords (v2)'].dropna().to_list()\n",
    "keywords = keywords.loc[:,['Keywords (students)','Keywords (protest)', 'Keywords (context)']]\n",
    "keywords.columns = ['student', 'protest', 'context']\n",
    "\n",
    "# function to score each article\n",
    "def get_score(text, url):\n",
    "    \n",
    "    # initialize dictionary to track subscores\n",
    "    scores = {\n",
    "        'student': False,\n",
    "        'protest': False,\n",
    "        'context': False\n",
    "    }\n",
    "    \n",
    "    def preprocess(text): \n",
    "        # remove everything but words and hyphenated words from text, then turn into list of words\n",
    "        text = text.strip().lower()\n",
    "        text = re.sub(r'\\W+', ' ', text).split(' ')\n",
    "        return text\n",
    "        \n",
    "        \n",
    "    def word_score(word): \n",
    "        for cat in keywords.columns:\n",
    "            # get keywords\n",
    "            keys = keywords[f'{cat}'].dropna().to_list()\n",
    "            # match to a category \n",
    "            for key in keys: \n",
    "                pattern = re.compile(fr'{key}')\n",
    "                match = pattern.fullmatch(word)\n",
    "                if match: \n",
    "                    # check category and only change if it hasn't been checked before \n",
    "                    if scores[f'{cat}'] == False: \n",
    "                        scores[f'{cat}'] = True\n",
    "    # preprocessing \n",
    "    text = preprocess(text)\n",
    "    \n",
    "    for word in text: \n",
    "        word_score(word)\n",
    "        \n",
    "    # finish this later\n",
    "    curr_score = sum(scores.values())\n",
    "    if curr_score > 0 and curr_score < 3:\n",
    "        try: \n",
    "            # get first two sentences of article\n",
    "            article = Article(url)\n",
    "\n",
    "            # preprocessing\n",
    "            article.download()\n",
    "            article.parse()\n",
    "\n",
    "            # get the first 250 characters\n",
    "            new_text = article.text[:250]\n",
    "            new_text = preprocess(new_text)\n",
    "\n",
    "            for word in new_text:\n",
    "                word_score(word)\n",
    "        except: \n",
    "            print('issue with retrieving article: ', url)\n",
    "        \n",
    "         \n",
    "    return sum(scores.values()) #student_score, protest_score, context_score\n",
    "\n",
    "# create a score column onto the data we collected\n",
    "df['score'] = df.apply(lambda row: get_score(row['title'], row['url']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc2fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes = test[test['score'] == 3]\n",
    "threes.groupby('url').first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0140712b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('filtered out', df.shape[0] - threes.shape[0], 'articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b7b71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threes.to_csv('final_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv('unfiltered.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccdc865",
   "metadata": {},
   "source": [
    "# Collecting from Specific News Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a01d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News Sites (v2)</th>\n",
       "      <th>Score (v2)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnn</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apnews</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wsj</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ft</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nbcnews</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  News Sites (v2)  Score (v2)\n",
       "0             cnn        -1.0\n",
       "1          apnews        -0.5\n",
       "2             wsj         0.5\n",
       "3              ft        -0.5\n",
       "4         nbcnews        -1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sites = pd.read_excel('news and keywords.xlsx').loc[:,['News Sites (v2)', 'Score (v2)']].dropna()\n",
    "sites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61e2baba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['breitbart',\n",
       " 'hannity',\n",
       " 'theblaze',\n",
       " 'heritage',\n",
       " 'washingtonexaminer',\n",
       " 'dailywire',\n",
       " 'thefederalist',\n",
       " 'thegatewaypundit',\n",
       " 'dailycaller',\n",
       " 'infowars',\n",
       " 'stanfordreview',\n",
       " 'thenewamerican',\n",
       " 'prntly']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "['breitbart',\n",
    " 'hannity',\n",
    " 'theblaze',\n",
    " 'washingtonexaminer',\n",
    " 'dailywire',\n",
    " 'thefederalist',\n",
    " 'thegatewaypundit',\n",
    " 'dailycaller',\n",
    " 'infowars',\n",
    " 'thenewamerican',\n",
    " 'prntly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6259a20c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  breitbart\n",
      "starting  hannity\n",
      "starting  theblaze\n",
      "starting  washingtonexaminer\n",
      "starting  dailywire\n",
      "starting  thefederalist\n",
      "starting  thegatewaypundit\n",
      "starting  dailycaller\n",
      "starting  infowars\n",
      "starting  thenewamerican\n",
      "starting  prntly\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m to_return\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(to_collect)): \n\u001b[0;32m---> 98\u001b[0m     site_df \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mto_collect\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m: \n\u001b[1;32m    101\u001b[0m         final_df \u001b[38;5;241m=\u001b[39m site_df\n",
      "Cell \u001b[0;32mIn[11], line 91\u001b[0m, in \u001b[0;36mcollect_news\u001b[0;34m(news_site)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# increment i \u001b[39;00m\n\u001b[1;32m     89\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 91\u001b[0m to_return[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mto_return\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(simple_score)\n\u001b[1;32m     92\u001b[0m to_return \u001b[38;5;241m=\u001b[39m to_return[to_return[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_return\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "keywords = pd.read_excel('news and keywords.xlsx')\n",
    "keywords = keywords.loc[:,['Keywords (students)','Keywords (protest)', 'Keywords (context)']]\n",
    "keywords.columns = ['student', 'protest', 'context']\n",
    "\n",
    "to_collect = sites.iloc[32:]['News Sites (v2)'].to_list()\n",
    "to_collect = ['breitbart',\n",
    " 'hannity',\n",
    " 'theblaze',\n",
    " 'washingtonexaminer',\n",
    " 'dailywire',\n",
    " 'thefederalist',\n",
    " 'thegatewaypundit',\n",
    " 'dailycaller',\n",
    " 'infowars',\n",
    " 'thenewamerican',\n",
    " 'prntly']\n",
    "\n",
    "# generate dates in 1 week intervals from April 01, 2024 to May 23rd, 2024\n",
    "dates = pd.date_range(start='4/1/2024', end='5/23/2024', freq='1W')\n",
    "# turn into tuples\n",
    "dates = [(date.year, date.month, date.day) for date in dates]\n",
    "\n",
    "\n",
    "# function to score each article\n",
    "def simple_score(text):\n",
    "    \n",
    "    # initialize dictionary to track subscores\n",
    "    scores = {\n",
    "        'student': False,\n",
    "        'protest': False,\n",
    "        'context': False\n",
    "    }\n",
    "    \n",
    "    def preprocess(text): \n",
    "        # remove everything but words and hyphenated words from text, then turn into list of words\n",
    "        text = text.strip().lower()\n",
    "        text = re.sub(r'\\W+', ' ', text).split(' ')\n",
    "        return text\n",
    "        \n",
    "        \n",
    "    def word_score(word): \n",
    "        for cat in keywords.columns:\n",
    "            # get keywords\n",
    "            keys = keywords[f'{cat}'].dropna().to_list()\n",
    "            # match to a category \n",
    "            for key in keys: \n",
    "                pattern = re.compile(fr'{key}')\n",
    "                match = pattern.fullmatch(word)\n",
    "                if match: \n",
    "                    # check category and only change if it hasn't been checked before \n",
    "                    if scores[f'{cat}'] == False: \n",
    "                        scores[f'{cat}'] = True\n",
    "    # preprocessing \n",
    "    text = preprocess(text)\n",
    "    \n",
    "    for word in text: \n",
    "        word_score(word)\n",
    "        \n",
    "         \n",
    "    return sum(scores.values())\n",
    "\n",
    "\n",
    "def collect_news(news_site): \n",
    "    print('starting ', news_site)\n",
    "    # for each date interval, search and collect data\n",
    "    for i in range(len(dates) - 1): # stop before the last date\n",
    "\n",
    "        # create gnews object \n",
    "        news = GNews(language='en', \n",
    "                        country='US', \n",
    "                        start_date= dates[i], \n",
    "                        end_date=dates[i + 1], \n",
    "                        max_results=50, \n",
    "                       )\n",
    "        # get news \n",
    "        results = news.get_news_by_site(f'{news_site}.com')\n",
    "        \n",
    "        # if this is the first iteration\n",
    "        if i == 0: \n",
    "            to_return = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "        # else add to existing df \n",
    "        else: \n",
    "            sub_results = pd.DataFrame(results)\n",
    "            to_return = pd.concat([to_return, sub_results])\n",
    "\n",
    "        # increment i \n",
    "        i += 1\n",
    "        \n",
    "    to_return['score'] = to_return['title'].apply(simple_score)\n",
    "    to_return = to_return[to_return['score'] == 3]\n",
    "    \n",
    "    \n",
    "    return to_return\n",
    "\n",
    "for i in range(len(to_collect)): \n",
    "    site_df = collect_news(to_collect[i])\n",
    "\n",
    "    if i == 0: \n",
    "        final_df = site_df\n",
    "    \n",
    "    else: \n",
    "        final_df = pd.concat([final_df, site_df])\n",
    "    \n",
    "    \n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71d87279",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df = pd.read_csv('final_dataset.csv', index_col=0)\n",
    "new_df = pd.concat([old_df, final_df])\n",
    "new_df.to_csv('final_dataset.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
